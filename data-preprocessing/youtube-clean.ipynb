{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder.appName('example application').getOrCreate()\n",
    "assert sys.version_info >= (3, 4) # make sure we have Python 3.4+\n",
    "assert spark.version >= '2.2' # make sure we have Spark 2.2+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine data into Single text file <Sources: Insta, FB, Twitter, FB>\n",
    "# No cleaning\n",
    "from pyspark.sql.types import StructField, StructType, StringType, ArrayType, IntegerType\n",
    "from pyspark.sql.functions import lower, col, udf, concat_ws, collect_list\n",
    "from pyspark.sql import Row\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, exists\n",
    "import json\n",
    "import re\n",
    "\n",
    "all_data = pd.read_csv('influencer_list.csv', sep=',')\n",
    "print(all_data.shape)\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "def get_handle(url):\n",
    "    splits = url.split('/')\n",
    "    if url:\n",
    "        if url.endswith('/'):\n",
    "            handle = splits[-2]\n",
    "        else:\n",
    "            handle = splits[-1]\n",
    "    else:\n",
    "        handle = None    \n",
    "    return handle\n",
    "\n",
    "tw_fields = ['screen_name', 'tweet_text', 'hashtags', 'favorites', 'retweet_count']\n",
    "tw_schema =  StructType([\n",
    "    StructField('screen_name', StringType(), True),\n",
    "    StructField('tweet_text', StringType(), True),\n",
    "    StructField('hashtags', ArrayType(StringType()), True),\n",
    "    StructField('favorites', IntegerType(), True),\n",
    "    StructField('retweet_count', IntegerType(), True),\n",
    "])\n",
    "\n",
    "ig_fields = ['twitter_handle', 'instagram_handle', 'likes', 'comments',\n",
    "         'hashtags', 'caption', 'timestamp', 'image_thumbnail']\n",
    "\n",
    "ig_schema =  StructType([\n",
    "    StructField('twitter_handle', StringType(), True),\n",
    "    StructField('instagram_handle', StringType(), True),\n",
    "    StructField('likes', IntegerType(), True),\n",
    "    StructField('comments', IntegerType(), True),\n",
    "    StructField('hashtags', ArrayType(StringType()), True),\n",
    "    StructField('caption', StringType(), True),\n",
    "    StructField('timestamp', StringType(), True),\n",
    "    StructField('image_thumbnail', StringType(), True),\n",
    "])\n",
    "\n",
    "fb_fields = ['twitter_handle', 'fb_handle', 'fb_name', 'fb_no_of_comments',\n",
    "         'fb_time_created', 'fb_description', 'fb_post_link', 'fb_img_link'\n",
    "         'fb_shares', 'fb_type']\n",
    "\n",
    "fb_schema =  StructType([\n",
    "    StructField('twitter_handle', StringType(), True),\n",
    "    StructField('fb_handle', StringType(), True),\n",
    "    StructField('fb_name', StringType(), True),\n",
    "    StructField('fb_no_of_comments', IntegerType(), True),\n",
    "    StructField('fb_time_created', StringType(), True),\n",
    "    StructField('fb_description', StringType(), True),\n",
    "    StructField('fb_post_link', StringType(), True),\n",
    "    StructField('fb_img_link', StringType(), True),\n",
    "    StructField('fb_shares', IntegerType(), True),\n",
    "    StructField('fb_type', StringType(), True),\n",
    "])\n",
    "\n",
    "yt_fields = ['twitter_handle', 'video_id', 'likes', 'dislikes',\n",
    "         'comments', 'views', 'title', 'description'\n",
    "         'tags', 'publishat', 'cc_filename']\n",
    "\n",
    "yt_schema =  StructType([\n",
    "    StructField('twitter_handle', StringType(), True),\n",
    "    StructField('video_id', StringType(), True),\n",
    "    StructField('likes', IntegerType(), True),\n",
    "    StructField('dislikes', IntegerType(), True),\n",
    "    StructField('comments', IntegerType(), True),\n",
    "    StructField('views', IntegerType(), True),\n",
    "    StructField('title', StringType(), True),\n",
    "    StructField('description', StringType(), True),\n",
    "    StructField('tags', ArrayType(StringType()), True),\n",
    "    StructField('publishat', StringType(), True),\n",
    "    StructField('cc_filename', StringType(), True),\n",
    "])\n",
    "\n",
    "yt_path = 'youtube-data'\n",
    "\n",
    "for index, row in all_data.iterrows():\n",
    "    tw_handle = get_handle(row['Twitter'])\n",
    "    errors = ['Dave2Dtv', 'susiebubble', 'ladolcevitablog', 'OMGitsfirefoxx', 'takahashimari',\n",
    "             'mannymua733','zoella']\n",
    "    if tw_handle in errors:\n",
    "        tw_path = 'twitter-data/{}'.format(tw_handle)\n",
    "        ig_path = 'instagram-data/{}'.format(tw_handle)\n",
    "        fb_path = 'facebook-data/{}'.format(tw_handle)\n",
    "\n",
    "        tw_df = spark.read.json(tw_path, schema=tw_schema).select('tweet_text')\n",
    "        tw_string = tw_df.agg(concat_ws(\". \", collect_list(tw_df.tweet_text))).alias('all_tweets')\n",
    "        if not exists(fb_path):        \n",
    "            ig_df = spark.read.json(yt_path, schema=ig_schema).select('caption')\n",
    "            ig_string = ig_df.agg(concat_ws(\". \", collect_list(ig_df.caption))).alias('all_captions')\n",
    "            unioned = tw_string.union(ig_string).alias('unioned')\n",
    "            print(\"No FB\")\n",
    "        elif not exists(ig_path):\n",
    "            fb_df = spark.read.json(fb_path, schema=fb_schema).select('fb_description')\n",
    "            fb_string = fb_df.agg(concat_ws(\". \", collect_list(fb_df.fb_description))).alias('all_descriptions')\n",
    "            unioned = tw_string.union(fb_string).alias('unioned')\n",
    "            print(\"No IG\")\n",
    "        else:\n",
    "            unioned = tw_string.union(ig_string).union(fb_string).alias('unioned')\n",
    "            print(\"All available\")\n",
    "\n",
    "        output_path = '3-combined-data/{}'.format(tw_handle)\n",
    "        print(\"Working on:{}\".format(tw_handle))\n",
    "        unioned.write.text(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, ArrayType, IntegerType\n",
    "from pyspark.sql.functions import lower, col, udf\n",
    "from pyspark.sql import Row\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, exists\n",
    "import json\n",
    "import re\n",
    "\n",
    "extended_stopwords = [word.strip().lower() for word in open('g10000.txt')]\n",
    "\n",
    "all_data = pd.read_csv('influencer_list.csv', sep=',')\n",
    "print(all_data.shape)\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "def get_handle(url):\n",
    "    splits = url.split('/')\n",
    "    if url:\n",
    "        if url.endswith('/'):\n",
    "            handle = splits[-2]\n",
    "        else:\n",
    "            handle = splits[-1]\n",
    "    else:\n",
    "        handle = None    \n",
    "    return handle\n",
    "\n",
    "\n",
    "category_dict = dict()\n",
    "for index, row in all_data.iterrows():\n",
    "    handle = get_handle(row['Twitter'])\n",
    "    category_dict[handle] = str(row['Category']).lower()\n",
    "\n",
    "\n",
    "youtube_words = ['youtube', 'yt', 'https', 'http'] #Need to add more if required\n",
    "fields = ['twitter_handle', 'video_id', 'likes', 'dislikes',\n",
    "         'comments', 'views', 'title', 'description'\n",
    "         'tags', 'publishat', 'cc_filename']\n",
    "\n",
    "schema =  StructType([\n",
    "    StructField('twitter_handle', StringType(), True),\n",
    "    StructField('video_id', StringType(), True),\n",
    "    StructField('likes', IntegerType(), True),\n",
    "    StructField('dislikes', IntegerType(), True),\n",
    "    StructField('comments', IntegerType(), True),\n",
    "    StructField('views', IntegerType(), True),\n",
    "    StructField('title', StringType(), True),\n",
    "    StructField('description', StringType(), True),\n",
    "    StructField('tags', ArrayType(StringType()), True),\n",
    "    StructField('publishat', StringType(), True),\n",
    "    StructField('cc_filename', StringType(), True),\n",
    "])\n",
    "\n",
    "def clean_cc(text):\n",
    "    text = text.lower()\n",
    "    if text:\n",
    "        cleaned = re.sub(\"(@[A-Za-z0-9_]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \", text).split()\n",
    "        cleaned_ws = [word for word in cleaned if word not in stopwords and word not in youtube_words \\\n",
    "                     and word not in extended_stopwords]\n",
    "        cleaned_wl = [lemma.lemmatize(word) for word in cleaned_ws]\n",
    "\n",
    "        cleaned_wl = ' '.join(cleaned_ws) \n",
    "        return cleaned_wl.strip()\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "stopwords = stopwords.words('english')\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "cleaner = udf(clean_cc, StringType())\n",
    "\n",
    "path = 'youtube-data-v2'\n",
    "youtube_files = [f for f in listdir(path) if isfile(join(path, f)) and (not f.startswith('.'))]\n",
    "\n",
    "for youtube_file in youtube_files:\n",
    "    print('Working on file: {}'.format(youtube_file))\n",
    "    df = spark.read.json('youtube-data-v2/{}'.format(youtube_file), schema=schema)\n",
    "    cc_files = list(df.select('cc_filename').collect())\n",
    "    category = category_dict[youtube_file]\n",
    "    print('Working on file: {} Category: {}'.format(youtube_file, category))\n",
    "    cc_path_prefix = '{}/others/{}/{}_cc/'.format(path, category, category)\n",
    "    output_path_prefix = cc_path_prefix.replace(category + '_cc', category + '_cc_cleaned')\n",
    "    output_directory = output_path_prefix + youtube_file + '/'\n",
    "    if not exists(output_directory):\n",
    "        makedirs(output_directory)\n",
    "    for cc_file in cc_files:\n",
    "        cc_name = cc_file['cc_filename']\n",
    "        cc_file_path = cc_path_prefix + cc_name + '.txt'\n",
    "        if exists(cc_file_path):\n",
    "            cc_txt = sc.textFile(cc_file_path)\n",
    "            cleaned_cc = cc_txt.map(clean_cc)\n",
    "            output_path = output_directory + cc_name\n",
    "            if not exists(output_path):\n",
    "                cleaned_cc.saveAsTextFile(output_path)\n",
    "            else:\n",
    "                print(\"NOTICE-EXISTS;{}\".format(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, ArrayType, IntegerType\n",
    "from pyspark.sql.functions import lower, col, udf\n",
    "from pyspark.sql import Row\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, exists\n",
    "import json\n",
    "import re\n",
    "\n",
    "all_data = pd.read_csv('influencer_list.csv', sep=',')\n",
    "print(all_data.shape)\n",
    "\n",
    "sc = spark.sparkContext\n",
    "# sc.hadoopConfiguration.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\n",
    "\n",
    "def get_handle(url):\n",
    "    splits = url.split('/')\n",
    "    if url:\n",
    "        if url.endswith('/'):\n",
    "            handle = splits[-2]\n",
    "        else:\n",
    "            handle = splits[-1]\n",
    "    else:\n",
    "        handle = None    \n",
    "    return handle\n",
    "\n",
    "\n",
    "category_dict = dict()\n",
    "for index, row in all_data.iterrows():\n",
    "    handle = get_handle(row['Twitter'])\n",
    "    category_dict[handle] = str(row['Category']).lower()\n",
    "\n",
    "\n",
    "youtube_words = ['youtube', 'yt', 'https', 'http'] #Need to add more if required\n",
    "fields = ['twitter_handle', 'video_id', 'likes', 'dislikes',\n",
    "         'comments', 'views', 'title', 'description'\n",
    "         'tags', 'publishat', 'cc_filename']\n",
    "\n",
    "schema =  StructType([\n",
    "    StructField('twitter_handle', StringType(), True),\n",
    "    StructField('video_id', StringType(), True),\n",
    "    StructField('likes', IntegerType(), True),\n",
    "    StructField('dislikes', IntegerType(), True),\n",
    "    StructField('comments', IntegerType(), True),\n",
    "    StructField('views', IntegerType(), True),\n",
    "    StructField('title', StringType(), True),\n",
    "    StructField('description', StringType(), True),\n",
    "    StructField('tags', ArrayType(StringType()), True),\n",
    "    StructField('publishat', StringType(), True),\n",
    "    StructField('cc_filename', StringType(), True),\n",
    "])\n",
    "\n",
    "path = 'youtube-data-v2'\n",
    "youtube_files = [f for f in listdir(path) if isfile(join(path, f)) and (not f.startswith('.'))]\n",
    "\n",
    "for youtube_file in youtube_files:\n",
    "    df = spark.read.json('youtube-data-v2/{}'.format(youtube_file), schema=schema)\n",
    "    cc_files = list(df.select('cc_filename').collect())\n",
    "    category = category_dict[youtube_file]\n",
    "    print('Working on file: {} Category: {}'.format(youtube_file, category))\n",
    "    cc_path_prefix = '{}/others/{}/{}_cc/'.format(path, category, category)\n",
    "    locations = list()\n",
    "    for cc_file in cc_files:\n",
    "        cc_name = cc_file['cc_filename']\n",
    "        cc_file_path = cc_path_prefix + cc_name + '.txt'\n",
    "        if exists(cc_file_path):\n",
    "            locations.append(cc_file_path)\n",
    "    loc_str = ','.join(locations)\n",
    "    sample_rdd = sc.textFile(loc_str)\n",
    "    output_loc = 'youtube-combined-cc/{}'.format(youtube_file)\n",
    "\n",
    "    try:\n",
    "        sample_rdd.coalesce(1).saveAsTextFile(output_loc)\n",
    "    except Exception:\n",
    "        print('error on: {}'.format(youtube_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_details = pd.read_csv('all_influencers_details.csv', sep=';')\n",
    "for index, row in all_details.iterrows():\n",
    "    all_locations = list()\n",
    "    tw_handle = row['tw_handle']\n",
    "    combined_3_path = '3-combined-data/{}/'.format(tw_handle)\n",
    "    for f in listdir(combined_3_path):\n",
    "        if isfile(join(combined_3_path, f)) and (not f.startswith('.')) and (not f.startswith('_')):\n",
    "            all_locations.append(join(combined_3_path, f))\n",
    "    youtube_comb_cc_path = 'youtube-combined-cc/{}/'.format(tw_handle)\n",
    "    if exists(youtube_comb_cc_path):\n",
    "        for f in listdir(youtube_comb_cc_path):\n",
    "            if isfile(join(youtube_comb_cc_path, f)) and (not f.startswith('.')) and (not f.startswith('_')):\n",
    "                all_locations.append(join(youtube_comb_cc_path, f))\n",
    "    locations_str = ','.join(all_locations)\n",
    "#     print(locations_str)\n",
    "    rdd = sc.textFile(locations_str)\n",
    "    output_loc_comb = 'combined-data-4-sources/{}'.format(tw_handle)\n",
    "    print(\"Working on:{}\".format(tw_handle))\n",
    "    try:\n",
    "        rdd.coalesce(1).saveAsTextFile(output_loc_comb)\n",
    "    except Exception:\n",
    "        print('error on: {}'.format(tw_handle))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#         res = selected.join(df, on=df['class_description'] == selected['vgg19_class_description']).orderBy('score')\n",
    "#         res.select('vgg19_class_description', 'score', 'image').drop_duplicates().orderBy('score').show()\n",
    "#         res = df.select('*').where(df['class_description'].isin(distinct_classes))\n",
    "\n",
    "#         vgg19_gpd = vgg19.groupby('class_description').agg(avg('score').alias('vgg19_score'))\n",
    "#         vgg16_gpd = vgg16.groupby('class_description').agg(avg('score').alias('vgg16_score'))\n",
    "\n",
    "# select vgg19 into different df; select vgg16 into different df\n",
    "# groupby object; agg=average score\n",
    "# join by object; avgscore-vgg16 and avgscore-vgg19;\n",
    "# combined score = weighted score for each object with 0.6 and 0.4 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pymongo.results.InsertManyResult object at 0x1141a4548>\n",
      "<pymongo.results.InsertManyResult object at 0x1141e4448>\n",
      "<pymongo.results.InsertManyResult object at 0x1139f9988>\n",
      "<pymongo.results.InsertManyResult object at 0x113903a48>\n",
      "<pymongo.results.InsertManyResult object at 0x113868208>\n",
      "<pymongo.results.InsertManyResult object at 0x1139fab48>\n",
      "<pymongo.results.InsertManyResult object at 0x1139fa408>\n",
      "<pymongo.results.InsertManyResult object at 0x11419e188>\n",
      "<pymongo.results.InsertManyResult object at 0x11414a648>\n",
      "<pymongo.results.InsertManyResult object at 0x11398e848>\n",
      "<pymongo.results.InsertManyResult object at 0x113cdf508>\n",
      "<pymongo.results.InsertManyResult object at 0x11418c388>\n",
      "<pymongo.results.InsertManyResult object at 0x10e7d0e08>\n",
      "<pymongo.results.InsertManyResult object at 0x1141a4548>\n",
      "<pymongo.results.InsertManyResult object at 0x114163648>\n",
      "<pymongo.results.InsertManyResult object at 0x113df4888>\n",
      "<pymongo.results.InsertManyResult object at 0x114186448>\n",
      "<pymongo.results.InsertManyResult object at 0x114158c08>\n",
      "<pymongo.results.InsertManyResult object at 0x1140f7c48>\n",
      "<pymongo.results.InsertManyResult object at 0x1141df908>\n",
      "<pymongo.results.InsertManyResult object at 0x113c80648>\n",
      "<pymongo.results.InsertManyResult object at 0x1138dd048>\n",
      "<pymongo.results.InsertManyResult object at 0x1141a1ac8>\n",
      "<pymongo.results.InsertManyResult object at 0x1138c2688>\n",
      "<pymongo.results.InsertManyResult object at 0x1140f7108>\n",
      "<pymongo.results.InsertManyResult object at 0x11399e108>\n",
      "<pymongo.results.InsertManyResult object at 0x11416f0c8>\n",
      "<pymongo.results.InsertManyResult object at 0x113878688>\n",
      "<pymongo.results.InsertManyResult object at 0x1139024c8>\n",
      "<pymongo.results.InsertManyResult object at 0x11417c588>\n",
      "<pymongo.results.InsertManyResult object at 0x1141fe588>\n",
      "<pymongo.results.InsertManyResult object at 0x11415d988>\n",
      "<pymongo.results.InsertManyResult object at 0x11399e6c8>\n",
      "<pymongo.results.InsertManyResult object at 0x114152988>\n",
      "<pymongo.results.InsertManyResult object at 0x114166348>\n",
      "<pymongo.results.InsertManyResult object at 0x113902708>\n",
      "<pymongo.results.InsertManyResult object at 0x1138e9fc8>\n",
      "<pymongo.results.InsertManyResult object at 0x1141a4588>\n",
      "<pymongo.results.InsertManyResult object at 0x10e7e0a88>\n",
      "<pymongo.results.InsertManyResult object at 0x1141d5908>\n",
      "<pymongo.results.InsertManyResult object at 0x1141a60c8>\n",
      "<pymongo.results.InsertManyResult object at 0x114158548>\n",
      "<pymongo.results.InsertManyResult object at 0x1138e9dc8>\n",
      "<pymongo.results.InsertManyResult object at 0x113994f08>\n",
      "<pymongo.results.InsertManyResult object at 0x10e7e0a48>\n",
      "<pymongo.results.InsertManyResult object at 0x113cee0c8>\n",
      "<pymongo.results.InsertManyResult object at 0x11414a608>\n",
      "<pymongo.results.InsertManyResult object at 0x1138c2ec8>\n",
      "<pymongo.results.InsertManyResult object at 0x1141a4a48>\n",
      "<pymongo.results.InsertManyResult object at 0x113a5b548>\n",
      "<pymongo.results.InsertManyResult object at 0x1138782c8>\n",
      "<pymongo.results.InsertManyResult object at 0x1141fa8c8>\n",
      "<pymongo.results.InsertManyResult object at 0x1138e9288>\n",
      "<pymongo.results.InsertManyResult object at 0x1138cf1c8>\n",
      "<pymongo.results.InsertManyResult object at 0x1139c4bc8>\n",
      "<pymongo.results.InsertManyResult object at 0x1138ff9c8>\n",
      "<pymongo.results.InsertManyResult object at 0x10e7d0588>\n",
      "<pymongo.results.InsertManyResult object at 0x10e7d0448>\n",
      "<pymongo.results.InsertManyResult object at 0x1140f7108>\n",
      "<pymongo.results.InsertManyResult object at 0x113cee0c8>\n",
      "<pymongo.results.InsertManyResult object at 0x1141e4608>\n",
      "<pymongo.results.InsertManyResult object at 0x1141e4448>\n",
      "<pymongo.results.InsertManyResult object at 0x1140a4488>\n",
      "<pymongo.results.InsertManyResult object at 0x114173ac8>\n",
      "<pymongo.results.InsertManyResult object at 0x1138e9708>\n",
      "<pymongo.results.InsertManyResult object at 0x1141a4988>\n",
      "<pymongo.results.InsertManyResult object at 0x1141c5948>\n",
      "<pymongo.results.InsertManyResult object at 0x114152708>\n",
      "<pymongo.results.InsertManyResult object at 0x1141a4408>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-b14ac4ef434c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'combined_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjoined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vgg19_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjoined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vgg16_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mcombined_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mtop5_pcile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_pd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'combined_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mselected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'combined_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mtop5_pcile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'combined_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mdistinct_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vgg19_class_description'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vgg19_class_description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mquantile\u001b[0;34m(self, q, axis, numeric_only, interpolation)\u001b[0m\n\u001b[1;32m   5325\u001b[0m                                      \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5326\u001b[0m                                      \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5327\u001b[0;31m                                      transposed=is_transposed)\n\u001b[0m\u001b[1;32m   5328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mquantile\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   3198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'quantile'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mreduction\u001b[0;34m(self, f, axis, consolidate, transposed, **kwargs)\u001b[0m\n\u001b[1;32m   3168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3169\u001b[0m         \u001b[0;31m# single block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3170\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concat_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3172\u001b[0m         \u001b[0;31m# compute the orderings of our original data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/dtypes/concat.py\u001b[0m in \u001b[0;36m_concat_compat\u001b[0;34m(to_concat, axis)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mto_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mto_concat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import StructField, StructType, StringType, ArrayType, IntegerType\n",
    "from pyspark.sql.functions import avg, count, col\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, exists\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.influencers_db\n",
    "collection = db.cv_collection\n",
    "\n",
    "image_rec_path = 'keras-vgg-export/reformatted/'\n",
    "img_vgg_schema =  StructType([\n",
    "    StructField('class_name', StringType(), True),\n",
    "    StructField('class_description', StringType(), True),\n",
    "    StructField('score', DoubleType(), True),\n",
    "    StructField('image', StringType(), True),\n",
    "    StructField('model', StringType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "keras_files = [f for f in listdir(image_rec_path) if isfile(join(image_rec_path, f)) and (not f.startswith('.'))]\n",
    "\n",
    "for keras_file in keras_files:\n",
    "#     if keras_file == 'MKBHD':\n",
    "    df = spark.read.csv(join(image_rec_path, keras_file), schema = img_vgg_schema, header=True)\n",
    "    vgg19 = df.select('*').where(df['model'] == 'VGG19').withColumnRenamed('class_description', 'vgg19_class_description')\n",
    "    vgg16 = df.select('*').where(df['model'] == 'VGG16').withColumnRenamed('class_description', 'vgg16_class_description')\n",
    "    vgg19_gpd = vgg19.groupby('vgg19_class_description').agg(count('*').alias('vgg19_count'))\n",
    "    vgg16_gpd = vgg16.groupby('vgg16_class_description').agg(count('*').alias('vgg16_count'))\n",
    "    joined = vgg19_gpd.join(vgg16_gpd, on=vgg16_gpd['vgg16_class_description'] == vgg19_gpd['vgg19_class_description'])\n",
    "    combined = joined.withColumn('combined_score', 0.6*joined['vgg19_count'] + 0.4*joined['vgg16_count'])\n",
    "    combined_pd = combined.toPandas()\n",
    "    top5_pcile = combined_pd.quantile(0.95)['combined_score']\n",
    "    selected = combined.select('*').where(combined['combined_score'] >= top5_pcile).orderBy('combined_score', ascending=False)\n",
    "    distinct_classes = selected.select('vgg19_class_description').distinct().toPandas()['vgg19_class_description'].tolist()\n",
    "    result = df.filter(col('class_description').isin(distinct_classes)).drop_duplicates(['class_description']).orderBy('score', ascending=False)\n",
    "    pd_Result = result.toPandas()\n",
    "    pd_Result['tw_handle'] = keras_file\n",
    "    #         print(pd_Result.head(10))\n",
    "    records = pd_Result.to_dict(orient='records')\n",
    "    op = collection.insert_many(records)\n",
    "    print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
