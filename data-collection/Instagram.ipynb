{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "\n",
    "\n",
    "def get_handle(url):\n",
    "    splits = url.split('/')\n",
    "    if url:\n",
    "        if url.endswith('/'):\n",
    "            handle = splits[-2]\n",
    "        else:\n",
    "            handle = splits[-1]\n",
    "    else:\n",
    "        handle = None    \n",
    "    return handle\n",
    "        \n",
    "df = pd.read_csv('influencer_list.csv')\n",
    "for index, row in df.iterrows():\n",
    "    influencer = row['Influencer']\n",
    "    category = row['Category']\n",
    "    \n",
    "    twitter_url = row['Twitter']\n",
    "    twitter_handle = get_handle(twitter_url)\n",
    "    \n",
    "    instagram_url = row['Instagram']\n",
    "    \n",
    "    if instagram_url != 'null':\n",
    "        instagram_handle = get_handle(instagram_url)\n",
    "        json_location = 'instagram_data/{}/{}/{}.json'.format(category.lower(), instagram_handle, instagram_handle)\n",
    "\n",
    "        f = open(json_location, 'r')\n",
    "        items = json.load(f)\n",
    "        f.close()\n",
    "        \n",
    "        outfile_location = 'instagram-data/{}'.format(twitter_handle)\n",
    "        with open(outfile_location, 'w') as insta_file:\n",
    "            for item in items:\n",
    "                post = dict()\n",
    "\n",
    "                post['twitter_handle'] = twitter_handle\n",
    "                post['instagram_handle'] = instagram_handle\n",
    "\n",
    "                try:\n",
    "                    post['likes'] = item['edge_media_preview_like']['count']\n",
    "                except (KeyError, IndexError) as err:\n",
    "                    post['likes'] = None\n",
    "\n",
    "                try:\n",
    "                    post['comments'] = item['edge_media_to_comment']['count']\n",
    "                except (KeyError, IndexError) as err:\n",
    "                    post['comments'] = None\n",
    "\n",
    "                try:\n",
    "                    post['hashtags'] = item['tags']\n",
    "                except (KeyError, IndexError) as err:\n",
    "                    post['hashtags'] = None\n",
    "\n",
    "                try:\n",
    "                    post['caption'] = item['edge_media_to_caption']['edges'][0]['node']['text']\n",
    "                except (KeyError, IndexError) as err:\n",
    "                    post['caption'] = None\n",
    "\n",
    "                try:\n",
    "                    post['timestamp'] = item['taken_at_timestamp']\n",
    "                except (KeyError, IndexError) as err:\n",
    "                    post['timestamp'] = None\n",
    "\n",
    "                try:\n",
    "                    post['image_thumbnail'] = item['thumbnail_src']\n",
    "                except (KeyError, IndexError) as err:\n",
    "                    post['image_thumbnail'] = None\n",
    "                \n",
    "                insta_file.write(json.dumps(post))\n",
    "                insta_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "page_content = requests.get(\"https://www.instagram.com/aringhosh/\").content\n",
    "soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "# for ultag in soup.find_all('ul', {'class': '_h9luf'}):\n",
    "#     for litag in ultag.find_all('li'):\n",
    "#         print(litag.text)\n",
    "\n",
    "# soup.find_all('ul', {'class': '_h9luf'})\n",
    "print(page_content.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          tw_handle             ig_handle ig_followers ig_following ig_posts  \\\n",
      "0        fitmencook            FitMenCook            1            2      511   \n",
      "1     yolanda_gampp         yolanda_gampp            2            8      841   \n",
      "2        izyhossack            izyhossack          221            9      795   \n",
      "3   NatashasKitchen       natashaskitchen          124            2      231   \n",
      "4      inspiralized          inspiralized          188            8      373   \n",
      "5    rosannapansino        rosannapansino            3            5      117   \n",
      "6    smittenkitchen        smittenkitchen          735            5        1   \n",
      "7         sweetambs             sweetambs            1            1      921   \n",
      "8   CupcakeAddictAU    mycupcakeaddiction            1            2      486   \n",
      "9       joythebaker           joythebaker          427            4      657   \n",
      "10            MKBHD                 MKBHD          298          710        3   \n",
      "11         ijustine              ijustine            1            4      993   \n",
      "12     UnboxTherapy          unboxtherapy            1            2      130   \n",
      "13         tldtoday              tldtoday          334            6      220   \n",
      "14        LinusTech             linustech          350            7        1   \n",
      "15      GuyKawasaki           guykawasaki           68            4       70   \n",
      "16  austinnotduncan       austinnotduncan          379           64      646   \n",
      "17      AndroidAuth      androidauthority          521            5       18   \n",
      "18         Dave2Dtv              dave2dtv           59            2       32   \n",
      "19    TechnoBuffalo         technobuffalo          339            6        9   \n",
      "20   chiaraferragni        chiaraferragni           12            5      736   \n",
      "21       weworewhat            weworewhat            1            7      995   \n",
      "22     GalMeetsGlam           juliahengel            1            1      392   \n",
      "23        gabifresh             gabifresh          601            6        1   \n",
      "24         imjennim              imjennim            1            6      760   \n",
      "25      susiebubble           susiebubble          411            7        4   \n",
      "26   nicolettemason        nicolettemason          164            3        2   \n",
      "27      ManRepeller         leandramcohen          632            9      851   \n",
      "28        aimeesong           songofstyle            4            7      901   \n",
      "29         bryanboy           bryanboycom          665            7      820   \n",
      "..              ...                   ...          ...          ...      ...   \n",
      "58  theblondeabroad       theblondeabroad          503            7      428   \n",
      "59      TravelBabbo           travelbabbo           66            9        2   \n",
      "60   iisuperwomanii        iisuperwomanii            7            3      961   \n",
      "61         kingbach              kingbach           15          414        3   \n",
      "62    camerondallas         camerondallas           20            8      452   \n",
      "63      shanedawson           shanedawson            5            9      192   \n",
      "64            harto                 harto            1            3      392   \n",
      "65        BabyAriel             babyariel            8            2      934   \n",
      "66     MirandaSings  mirandasingsofficial            6            5        1   \n",
      "67      tyleroakley           tyleroakley            6            4      901   \n",
      "68        loganpaul             loganpaul           16            1      169   \n",
      "69         lelepons              lelepons           23            6        1   \n",
      "70           dantdm                dantdm            2            8       61   \n",
      "71  Jack_Septic_Eye         jacksepticeye            5            5       61   \n",
      "72        pewdiepie             pewdiepie           13            9       79   \n",
      "74     sssniperwolf          sssniperwolf            2            9      174   \n",
      "75     ProSyndicate   thesyndicateproject           25            5        0   \n",
      "77       markiplier        markipliergram            6            5       39   \n",
      "78   OMGitsfirefoxx        omgitsfirefoxx          436            3      622   \n",
      "79    takahashimari            atomicmari          756            3      545   \n",
      "80  nikkietutorials       nikkietutorials            9            9      571   \n",
      "81  christendtweets    dominiquecosmetics          110            6        1   \n",
      "82       beautylish            beautylish          389            7      983   \n",
      "83      mannymua733           mannymua733            4            5      298   \n",
      "84         xoShaaan              shaaanxo            1            5      480   \n",
      "85    kandeejohnson         kandeejohnson            1            7      830   \n",
      "86       hudabeauty            hudabeauty           25          271       12   \n",
      "87     michellephan          michellephan            2            1        1   \n",
      "88      JeffreeStar           jeffreestar            5            7      305   \n",
      "89           zoella                zoella           11          539        1   \n",
      "\n",
      "                                     ig_thumbnail_url  \n",
      "0   https://scontent-sea1-1.cdninstagram.com/vp/a4...  \n",
      "1   https://scontent-sea1-1.cdninstagram.com/vp/64...  \n",
      "2   https://scontent-sea1-1.cdninstagram.com/vp/21...  \n",
      "3   https://scontent-sea1-1.cdninstagram.com/vp/7e...  \n",
      "4   https://scontent-sea1-1.cdninstagram.com/vp/3e...  \n",
      "5   https://scontent-sea1-1.cdninstagram.com/vp/b5...  \n",
      "6   https://scontent-sea1-1.cdninstagram.com/vp/2f...  \n",
      "7   https://scontent-sea1-1.cdninstagram.com/vp/30...  \n",
      "8   https://scontent-sea1-1.cdninstagram.com/vp/7a...  \n",
      "9   https://scontent-sea1-1.cdninstagram.com/vp/a3...  \n",
      "10  https://scontent-sea1-1.xx.fbcdn.net/v/t1.0-1/...  \n",
      "11  https://scontent-sea1-1.cdninstagram.com/vp/67...  \n",
      "12  https://scontent-sea1-1.cdninstagram.com/vp/e5...  \n",
      "13  https://scontent-sea1-1.cdninstagram.com/vp/34...  \n",
      "14  https://scontent-sea1-1.cdninstagram.com/vp/25...  \n",
      "15  https://scontent-sea1-1.cdninstagram.com/vp/dc...  \n",
      "16  https://scontent-sea1-1.cdninstagram.com/vp/b4...  \n",
      "17  https://scontent-sea1-1.cdninstagram.com/vp/92...  \n",
      "18  https://scontent-sea1-1.cdninstagram.com/vp/8a...  \n",
      "19  https://scontent-sea1-1.cdninstagram.com/vp/18...  \n",
      "20  https://scontent-sea1-1.cdninstagram.com/vp/e4...  \n",
      "21  https://scontent-sea1-1.cdninstagram.com/vp/2a...  \n",
      "22  https://scontent-sea1-1.cdninstagram.com/vp/e7...  \n",
      "23  https://scontent-sea1-1.cdninstagram.com/vp/ae...  \n",
      "24  https://scontent-sea1-1.cdninstagram.com/vp/63...  \n",
      "25  https://scontent-sea1-1.cdninstagram.com/vp/ab...  \n",
      "26  https://scontent-sea1-1.cdninstagram.com/vp/73...  \n",
      "27  https://scontent-sea1-1.cdninstagram.com/vp/f8...  \n",
      "28  https://scontent-sea1-1.cdninstagram.com/vp/c1...  \n",
      "29  https://scontent-sea1-1.cdninstagram.com/vp/17...  \n",
      "..                                                ...  \n",
      "58  https://scontent-sea1-1.cdninstagram.com/vp/93...  \n",
      "59  https://scontent-sea1-1.cdninstagram.com/vp/68...  \n",
      "60  https://scontent-sea1-1.cdninstagram.com/vp/9e...  \n",
      "61  https://scontent-sea1-1.cdninstagram.com/vp/29...  \n",
      "62  https://scontent-sea1-1.cdninstagram.com/vp/55...  \n",
      "63  https://scontent-sea1-1.cdninstagram.com/vp/ba...  \n",
      "64  https://scontent-sea1-1.cdninstagram.com/vp/61...  \n",
      "65  https://scontent-sea1-1.cdninstagram.com/vp/61...  \n",
      "66  https://scontent-sea1-1.cdninstagram.com/vp/ed...  \n",
      "67  https://scontent-sea1-1.cdninstagram.com/vp/be...  \n",
      "68  https://scontent-sea1-1.cdninstagram.com/vp/aa...  \n",
      "69  https://scontent-sea1-1.cdninstagram.com/vp/0f...  \n",
      "70  https://scontent-sea1-1.cdninstagram.com/vp/19...  \n",
      "71  https://scontent-sea1-1.cdninstagram.com/vp/1a...  \n",
      "72  https://scontent-sea1-1.cdninstagram.com/vp/d2...  \n",
      "74  https://scontent-sea1-1.cdninstagram.com/vp/91...  \n",
      "75  https://scontent-sea1-1.cdninstagram.com/vp/f5...  \n",
      "77  https://scontent-sea1-1.cdninstagram.com/vp/b4...  \n",
      "78  https://scontent-sea1-1.cdninstagram.com/vp/02...  \n",
      "79  https://scontent-sea1-1.cdninstagram.com/vp/90...  \n",
      "80  https://scontent-sea1-1.cdninstagram.com/vp/7a...  \n",
      "81  https://scontent-sea1-1.cdninstagram.com/vp/65...  \n",
      "82  https://scontent-sea1-1.cdninstagram.com/vp/f2...  \n",
      "83  https://scontent-sea1-1.cdninstagram.com/vp/94...  \n",
      "84  https://scontent-sea1-1.cdninstagram.com/vp/75...  \n",
      "85  https://scontent-sea1-1.cdninstagram.com/vp/96...  \n",
      "86  https://scontent-sea1-1.cdninstagram.com/vp/b9...  \n",
      "87  https://scontent-sea1-1.cdninstagram.com/vp/ba...  \n",
      "88  https://scontent-sea1-1.cdninstagram.com/vp/cc...  \n",
      "89  https://scontent-sea1-1.cdninstagram.com/vp/61...  \n",
      "\n",
      "[88 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "\n",
    "\n",
    "def get_handle(url):\n",
    "    splits = url.split('/')\n",
    "    if url:\n",
    "        if url.endswith('/'):\n",
    "            handle = splits[-2]\n",
    "        else:\n",
    "            handle = splits[-1]\n",
    "    else:\n",
    "        handle = None    \n",
    "    return handle\n",
    "        \n",
    "columns = ['tw_handle','ig_handle','ig_followers', 'ig_following', 'ig_posts', 'ig_thumbnail_url']\n",
    "outdf = pd.DataFrame(columns=columns)\n",
    "\n",
    "    \n",
    "df = pd.read_csv('influencer_list.csv')\n",
    "for index, row in df.iterrows():\n",
    "    influencer = row['Influencer']\n",
    "    category = row['Category']\n",
    "    \n",
    "    twitter_url = row['Twitter']\n",
    "    twitter_handle = get_handle(twitter_url)\n",
    "    \n",
    "    instagram_url = row['Instagram']\n",
    "\n",
    "    if instagram_url != 'null':\n",
    "        instagram_handle = get_handle(instagram_url)\n",
    "        \n",
    "        response = requests.get(instagram_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        metas = soup.find_all('meta')\n",
    "        description = [ meta.attrs['content'] for meta in metas if 'name' in meta.attrs and meta.attrs['name'] == 'description' ][0]\n",
    "        numbers = re.findall(r'\\d+', description)\n",
    "\n",
    "        followers = numbers[0]\n",
    "        following = numbers[1]\n",
    "        posts = numbers[2]\n",
    "\n",
    "        thumbnail_url = soup.find(\"meta\",  property=\"og:image\")['content']\n",
    "\n",
    "        result_list = [twitter_handle, instagram_handle, followers, following, posts, thumbnail_url]\n",
    "        outdf.loc[index] = result_list\n",
    "\n",
    "# outdf.to_csv('instagram_influencers_details.csv', sep=';', index=False)\n",
    "print(outdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image downloader\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "LOG_FILENAME = 'instagram_downloader_log.out'\n",
    "logging.basicConfig(\n",
    "    filename=LOG_FILENAME,\n",
    "    level=logging.ERROR,\n",
    ")\n",
    "\n",
    "path = 'instagram-data'\n",
    "instagram_files = [f for f in listdir(path) if isfile(join(path, f)) and (not f.startswith('.'))]\n",
    "\n",
    "counter = 0\n",
    "for instagram_file in instagram_files:\n",
    "    file_name = join(path, instagram_file)\n",
    "    df = pd.read_json(file_name, lines=True)\n",
    "    directory = 'instagram-images/{}'.format(instagram_file)\n",
    "    if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    for index, row in df.iterrows():\n",
    "        image_url = row['image_thumbnail']\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        if not response.ok:\n",
    "            logging.error('Response {};Influencer:{};URL:{}'.format(\n",
    "            response, instagram_file, image_url))\n",
    "        else:\n",
    "            with open('instagram-images/{}/{}_{}.jpg'.format(instagram_file, instagram_file, index), 'wb') as handle:\n",
    "                for block in response.iter_content(1024):\n",
    "                    if not block:\n",
    "                        break\n",
    "                    handle.write(block)\n",
    "#             logging.debug('SUCCESS;Influencer:{};URL:{};index:{}'.format(\n",
    "#             response, instagram_file, image_url, index))\n",
    "    counter += 1\n",
    "    print(\"Done for: {}\".format(instagram_file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
